# Руководство по написанию отчета

## Структура отчета

### 1. Листинг разработанного ядра и описание алгоритма распараллеливания

#### 1.1. Naive версия (базовая)

**Файл:** `transpose_cuda.cu`, строки 6-17

**Листинг ядра:**
```cuda
__global__ void transpose_naive_kernel(const float* in, float* out, int rows, int cols) {
    // Вычисляем глобальные индексы в исходной матрице
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    // Проверяем границы (для обработки размеров, не кратных размеру блока)
    if (row < rows && col < cols) {
        // Транспонирование: out[j][i] = in[i][j]
        // out имеет размер cols x rows
        out[col * rows + row] = in[row * cols + col];
    }
}
```

**Описание алгоритма:**
- Каждый поток обрабатывает один элемент матрицы
- Используется 2D grid и 2D блоки для естественного соответствия структуре матрицы
- Поток с координатами (row, col) читает элемент `in[row][col]` и записывает его в `out[col][row]`
- Прямой доступ к глобальной памяти без оптимизаций

**Иллюстрация:**
```
Исходная матрица (rows × cols):        Транспонированная (cols × rows):
┌─────────────┐                        ┌─────────────┐
│ in[0][0]    │                        │ out[0][0]   │
│ in[0][1]    │  ──транспонирование──> │ out[1][0]   │
│ in[1][0]    │                        │ out[0][1]   │
│ in[1][1]    │                        │ out[1][1]   │
└─────────────┘                        └─────────────┘

Поток (row=0, col=1): in[0][1] → out[1][0]
Поток (row=1, col=0): in[1][0] → out[0][1]
```

#### 1.2. Оптимизированная версия с Shared Memory

**Файл:** `transpose_cuda.cu`, строки 20-60

**Листинг ядра:**
```cuda
__global__ void transpose_shared_kernel(const float* in, float* out, int rows, int cols) {
    const int TILE_DIM = 32;
    const int BLOCK_ROWS = 8;
    
    // Shared memory с padding для избежания bank conflicts
    __shared__ float tile[TILE_DIM][TILE_DIM + 1];
    
    // Вычисляем глобальные индексы в исходной матрице
    int col = blockIdx.x * TILE_DIM + threadIdx.x;
    int row = blockIdx.y * BLOCK_ROWS + threadIdx.y;
    
    // Читаем тайл в shared memory коалесцированно
    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {
        int read_row = row + i;
        if (read_row < rows && col < cols) {
            tile[threadIdx.y + i][threadIdx.x] = in[read_row * cols + col];
        }
    }
    
    __syncthreads();
    
    // Вычисляем глобальные индексы для записи (транспонированные)
    int col_out = blockIdx.y * TILE_DIM + threadIdx.x;
    int row_out = blockIdx.x * BLOCK_ROWS + threadIdx.y;
    
    // Записываем из shared memory в глобальную память транспонированно
    for (int i = 0; i < TILE_DIM; i += BLOCK_ROWS) {
        int write_row = row_out + i;
        if (write_row < cols && col_out < rows) {
            out[write_row * rows + col_out] = tile[threadIdx.x][threadIdx.y + i];
        }
    }
}
```

**Описание алгоритма:**
1. **Разбиение на тайлы:** Матрица разбивается на тайлы размером 32×32 элемента
2. **Коалесцированное чтение:** Потоки блока читают тайл из глобальной памяти в shared memory коалесцированно (соседние потоки читают соседние элементы)
3. **Синхронизация:** `__syncthreads()` гарантирует, что все потоки блока завершили чтение
4. **Транспонирование в shared memory:** При записи индексы меняются местами (threadIdx.x ↔ threadIdx.y)
5. **Коалесцированная запись:** Запись в глобальную память также коалесцирована
6. **Padding:** Использование `[TILE_DIM+1]` вместо `[TILE_DIM]` предотвращает bank conflicts в shared memory

**Иллюстрация работы с тайлами:**
```
Исходная матрица разбита на тайлы 32×32:

┌─────────────┬─────────────┐
│   Тайл 0,0  │   Тайл 0,1  │
│  (32×32)    │  (32×32)    │
├─────────────┼─────────────┤
│   Тайл 1,0  │   Тайл 1,1  │
│  (32×32)    │  (32×32)    │
└─────────────┴─────────────┘

Процесс транспонирования тайла:
1. Чтение тайла в shared memory (коалесцированно)
2. Транспонирование в shared memory (меняем индексы)
3. Запись транспонированного тайла (коалесцированно)
```

**Преимущества оптимизации:**
- **Коалесцированный доступ:** Соседние потоки читают/пишут соседние элементы → эффективное использование пропускной способности памяти
- **Shared memory:** Быстрая локальная память вместо медленной глобальной
- **Отсутствие bank conflicts:** Padding предотвращает конфликты банков памяти

---

### 2. Графики зависимости времени работы

**Файл с данными:** `results.csv`

**Формат данных:**
```
size,cpu_time_ms,gpu_naive_time_ms,gpu_shared_time_ms,speedup_naive,speedup_shared,correct
256,0.123456,0.045678,0.012345,2.70,10.00,1
512,0.456789,0.123456,0.034567,3.70,13.21,1
...
```

**Скрипт для построения графика:** `plot_results.py`

**Запуск:**
```bash
python plot_results.py
```

**Результат:** График сохраняется в `transpose_plot.png`

**Что показать в отчете:**
- График зависимости времени выполнения от размера матрицы (log-log шкала)
- Три кривые: CPU, GPU naive, GPU shared memory
- Анализ ускорения для разных размеров матриц
- Объяснение, почему shared memory версия быстрее

---

### 3. Способ вычисления количества блоков и потоков

#### 3.1. Naive версия

**Файл:** `transpose_cuda.cu`, строки 95-103

**Код:**
```c
const int TILE_SIZE = 32;  // Размер блока 32×32
dim3 blockSize(TILE_SIZE, TILE_SIZE);  // Блок: 32×32 = 1024 потока

// Вычисляем количество блоков в grid (с округлением вверх)
int gridX = (cols + TILE_SIZE - 1) / TILE_SIZE;
int gridY = (rows + TILE_SIZE - 1) / TILE_SIZE;
dim3 gridSize(gridX, gridY);
```

**Формула:**
- **Потоков в блоке:** `TILE_SIZE × TILE_SIZE = 32 × 32 = 1024 потока`
- **Блоков по X:** `gridX = ⌈cols / TILE_SIZE⌉` (округление вверх)
- **Блоков по Y:** `gridY = ⌈rows / TILE_SIZE⌉` (округление вверх)
- **Всего потоков:** `gridX × gridY × 1024`

**Пример для матрицы 1000×1000:**
- `gridX = (1000 + 32 - 1) / 32 = 1024 / 32 = 32 блока`
- `gridY = (1000 + 32 - 1) / 32 = 32 блока`
- Всего: 32 × 32 = 1024 блока
- Всего потоков: 1024 × 1024 = 1,048,576 потоков

#### 3.2. Shared Memory версия

**Файл:** `transpose_cuda.cu`, строки 183-185

**Код:**
```c
const int TILE_DIM = 32;
const int BLOCK_ROWS = 8;
dim3 dimBlock(TILE_DIM, BLOCK_ROWS);  // Блок: 32×8 = 256 потоков

dim3 dimGrid((cols + TILE_DIM - 1) / TILE_DIM, 
             (rows + TILE_DIM - 1) / BLOCK_ROWS);
```

**Формула:**
- **Потоков в блоке:** `TILE_DIM × BLOCK_ROWS = 32 × 8 = 256 потоков`
- **Блоков по X:** `gridX = ⌈cols / TILE_DIM⌉`
- **Блоков по Y:** `gridY = ⌈rows / TILE_DIM⌉` (используется TILE_DIM, а не BLOCK_ROWS!)
- **Всего потоков:** `gridX × gridY × 256`

**Почему BLOCK_ROWS = 8?**
- В блоке 256 потоков (32×8)
- Каждый поток обрабатывает несколько элементов в цикле
- Это позволяет обработать тайл 32×32 с меньшим количеством потоков

**Пример для матрицы 1000×1000:**
- `gridX = (1000 + 32 - 1) / 32 = 32 блока`
- `gridY = (1000 + 32 - 1) / 32 = 32 блока`
- Всего: 32 × 32 = 1024 блока
- Всего потоков: 1024 × 256 = 262,144 потоков

**Важно:** Округление вверх `(N + TILE_SIZE - 1) / TILE_SIZE` гарантирует, что все элементы матрицы будут обработаны, даже если размер не кратен размеру тайла.

---

### 4. Листинги разработанных программ

#### 4.1. Основная программа (main.c)

**Файл:** `main.c` (140 строк)

**Основные функции:**
- Инициализация тестовых данных
- Запуск тестов для разных размеров матриц (256, 512, 1024, 2048, 4096, 8192)
- Измерение времени выполнения (10 прогонов для усреднения)
- Сравнение результатов CPU и GPU
- Сохранение результатов в CSV

#### 4.2. CPU реализация

**Файл:** `transpose_cpu.c` (10 строк)
**Заголовок:** `transpose_cpu.h`

**Алгоритм:** Двойной цикл по всем элементам матрицы
```c
void transpose_cpu(const float* in, float* out, int rows, int cols) {
    for (int i = 0; i < rows; i++) {
        for (int j = 0; j < cols; j++) {
            out[j * rows + i] = in[i * cols + j];
        }
    }
}
```

#### 4.3. CUDA реализация

**Файл:** `transpose_cuda.cu` (313 строк)
**Заголовок:** `transpose_cuda.cuh`

**Содержит:**
- `transpose_naive_kernel` - базовое ядро
- `transpose_shared_kernel` - оптимизированное ядро
- `transpose_gpu_naive` - host функция для naive версии
- `transpose_gpu_shared` - host функция для shared memory версии

#### 4.4. Вспомогательные функции

**Файл:** `utils.c` (105 строк)
**Заголовок:** `utils.h`

**Функции:**
- `create_matrix` - создание и заполнение матрицы случайными значениями
- `free_matrix` - освобождение памяти
- `print_matrix` - печать матрицы (для маленьких размеров)
- `matrices_equal` - сравнение матриц с погрешностью
- `get_time_ms` - измерение времени (кроссплатформенное)

#### 4.5. Информация об устройстве

**Файл:** `device_info.cu` (35 строк)

**Функции:**
- `is_cuda_available` - проверка доступности CUDA
- `print_device_info` - вывод информации о GPU
- `cudaDeviceReset_safe` - безопасный сброс устройства

---

## Структура отчета (рекомендуемая)

1. **Введение**
   - Цель работы
   - Описание задачи транспонирования матрицы

2. **Алгоритм распараллеливания**
   - Описание naive версии с иллюстрацией
   - Описание оптимизированной версии с shared memory
   - Сравнение подходов

3. **Вычисление блоков и потоков**
   - Формулы для naive версии
   - Формулы для shared memory версии
   - Примеры расчетов

4. **Результаты экспериментов**
   - График зависимости времени от размера матрицы
   - Таблица результатов (из results.csv)
   - Анализ ускорения

5. **Листинги программ**
   - main.c
   - transpose_cpu.c
   - transpose_cuda.cu (ядра и host функции)
   - utils.c (основные функции)

6. **Выводы**
   - Эффективность распараллеливания
   - Преимущества shared memory подхода
   - Зависимость ускорения от размера матрицы

---

## Дополнительные материалы

- **Скрипт построения графика:** `plot_results.py`
- **Результаты измерений:** `results.csv`
- **Документация по компиляции:** `COMPILE_GUIDE.md`

